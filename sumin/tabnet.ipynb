{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tabnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 패키지 불러오기\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import f1_score\n",
    "import torch\n",
    "\n",
    "import optuna\n",
    "from optuna import Trial, visualization\n",
    "from optuna.samplers import TPESampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 불러오기\n",
    "x_train = pd.read_csv(\"C:/Users/sumin/Bitamin/2학기/conference/bitamin_ChurnProject-1/data/fill_data/x_train.csv\")\n",
    "x_valid = pd.read_csv(\"C:/Users/sumin/Bitamin/2학기/conference/bitamin_ChurnProject-1/data/fill_data/x_valid.csv\")\n",
    "y_train = pd.read_csv(\"C:/Users/sumin/Bitamin/2학기/conference/bitamin_ChurnProject-1/data/fill_data/y_train.csv\")\n",
    "y_valid = pd.read_csv(\"C:/Users/sumin/Bitamin/2학기/conference/bitamin_ChurnProject-1/data/fill_data/y_valid.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.values\n",
    "x_valid = x_valid.values\n",
    "y_train = y_train.values\n",
    "y_valid = y_valid.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.array(y_train).ravel()\n",
    "y_valid = np.array(y_valid).ravel()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 39 with best_epoch = 10 and best_valid_accuracy = 0.82578\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sumin\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "걸린 시간 : 869.9879610538483\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "clf = TabNetClassifier(\n",
    "    n_d=56, \n",
    "    n_steps=3, gamma=1.4,\n",
    "    lambda_sparse=1.1694994589239397e-05, \n",
    "    optimizer_fn=torch.optim.Adam,\n",
    "    optimizer_params=dict(lr=2e-2, weight_decay=1e-5),\n",
    "    mask_type='sparsemax',\n",
    "    n_shared=2,\n",
    "    scheduler_params=dict(mode=\"min\",\n",
    "        patience = 3, # changing sheduler patience to be lower than early stopping patience\n",
    "        min_lr=1e-5,factor=0.5,),\n",
    "    scheduler_fn=torch.optim.lr_scheduler.ReduceLROnPlateau,\n",
    "    verbose=0)\n",
    "clf.fit(x_train, y_train,\n",
    "    eval_set=[(x_train, y_train),(x_valid, y_valid)],\n",
    "    eval_name=['train', 'valid'],\n",
    "    eval_metric=['accuracy'],\n",
    "    drop_last = False,\n",
    "    patience = 29, max_epochs = 80\n",
    ")\n",
    "end_time = time.time()\n",
    "print('걸린 시간 :', end_time - start_time)\n",
    "\n",
    "## 기본값: 0.81477"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.83\n",
      "[[ 2412 16731]\n",
      " [  336 79678]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        -1.0       0.88      0.13      0.22     19143\n",
      "         1.0       0.83      1.00      0.90     80014\n",
      "\n",
      "    accuracy                           0.83     99157\n",
      "   macro avg       0.85      0.56      0.56     99157\n",
      "weighted avg       0.84      0.83      0.77     99157\n",
      "\n",
      "0.8278790201397784\n",
      "Accuracy: 0.83\n",
      "[[ 190 1361]\n",
      " [  31 6408]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        -1.0       0.86      0.12      0.21      1551\n",
      "         1.0       0.82      1.00      0.90      6439\n",
      "\n",
      "    accuracy                           0.83      7990\n",
      "   macro avg       0.84      0.56      0.56      7990\n",
      "weighted avg       0.83      0.83      0.77      7990\n",
      "\n",
      "0.8257822277847309\n"
     ]
    }
   ],
   "source": [
    "# prediction and evaluation\n",
    "# train data\n",
    "train_pred_y = clf.predict(x_train)\n",
    "print('Accuracy: {:.2f}'.format(accuracy_score(y_train, train_pred_y)))\n",
    "print(confusion_matrix(y_train, train_pred_y))\n",
    "print(classification_report(y_train, train_pred_y))\n",
    "print(f1_score(y_train, train_pred_y, average='micro'))\n",
    "\n",
    "# valid data\n",
    "valid_pred_y = clf.predict(x_valid)\n",
    "print('Accuracy: {:.2f}'.format(accuracy_score(y_valid, valid_pred_y)))\n",
    "print(confusion_matrix(y_valid, valid_pred_y))\n",
    "print(classification_report(y_valid, valid_pred_y))\n",
    "print(f1_score(y_valid, valid_pred_y, average='micro'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "하이퍼파라미터 튜닝"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Objective(trial : Trial, x_train, y_train, x_valid, y_valid):\n",
    "    mask_type = trial.suggest_categorical(\"mask_type\", [\"entmax\", \"sparsemax\"])\n",
    "    n_da = trial.suggest_int(\"n_da\", 56, 64, step=4)\n",
    "    n_steps = trial.suggest_int(\"n_steps\", 1, 3, step=1)\n",
    "    gamma = trial.suggest_float(\"gamma\", 1., 1.4, step=0.2)\n",
    "    n_shared = trial.suggest_int(\"n_shared\", 1, 3)\n",
    "    lambda_sparse = trial.suggest_float(\"lambda_sparse\", 1e-6, 1e-3, log=True)\n",
    "    tabnet_params = dict(n_d=n_da, n_a=n_da, n_steps=n_steps, gamma=gamma,\n",
    "        lambda_sparse=lambda_sparse, optimizer_fn=torch.optim.Adam,\n",
    "        optimizer_params=dict(lr=2e-2, weight_decay=1e-5),\n",
    "        mask_type=mask_type, n_shared=n_shared,\n",
    "        scheduler_params=dict(mode=\"min\",\n",
    "        patience=trial.suggest_int(\"patienceScheduler\",low=3,high=10), # changing sheduler patience to be lower than early stopping patience\n",
    "        min_lr=1e-5,factor=0.5,),\n",
    "        scheduler_fn=torch.optim.lr_scheduler.ReduceLROnPlateau,\n",
    "        verbose=0,\n",
    "        ) #early stopping\n",
    "    \n",
    "    clf = TabNetClassifier(**tabnet_params)\n",
    "    clf.fit(x_train, y_train, eval_set=[(x_train, y_train), (x_valid, y_valid)],\n",
    "        patience=trial.suggest_int(\"patience\",low=15,high=30), max_epochs=trial.suggest_int('epochs', 1, 100),\n",
    "        eval_metric=['accuracy'])\n",
    "\n",
    "    score = f1_score(clf.predict(x_valid), y_valid, average='micro')\n",
    "    \n",
    "    return score \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-01-30 15:36:09,979]\u001b[0m A new study created in memory with name: TabNet optimization\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 34 with best_epoch = 9 and best_val_1_accuracy = 0.83492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sumin\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "\u001b[32m[I 2023-01-30 15:59:27,052]\u001b[0m Trial 0 finished with value: 0.8349186483103879 and parameters: {'mask_type': 'sparsemax', 'n_da': 64, 'n_steps': 3, 'gamma': 1.0, 'n_shared': 2, 'lambda_sparse': 5.39549495923216e-06, 'patienceScheduler': 10, 'patience': 25, 'epochs': 41}. Best is trial 0 with value: 0.8349186483103879.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 24 with best_epoch = 6 and best_val_1_accuracy = 0.83091\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sumin\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "\u001b[32m[I 2023-01-30 16:11:32,665]\u001b[0m Trial 1 finished with value: 0.8309136420525657 and parameters: {'mask_type': 'entmax', 'n_da': 64, 'n_steps': 2, 'gamma': 1.0, 'n_shared': 2, 'lambda_sparse': 2.8289851088977512e-05, 'patienceScheduler': 6, 'patience': 18, 'epochs': 73}. Best is trial 0 with value: 0.8349186483103879.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 34 with best_epoch = 8 and best_val_1_accuracy = 0.82315\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sumin\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "\u001b[32m[I 2023-01-30 16:23:25,821]\u001b[0m Trial 2 finished with value: 0.8231539424280351 and parameters: {'mask_type': 'entmax', 'n_da': 64, 'n_steps': 1, 'gamma': 1.2, 'n_shared': 1, 'lambda_sparse': 0.0008506014736380754, 'patienceScheduler': 9, 'patience': 26, 'epochs': 38}. Best is trial 0 with value: 0.8349186483103879.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop training because you reached max_epochs = 7 with best_epoch = 4 and best_val_1_accuracy = 0.81176\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sumin\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "\u001b[32m[I 2023-01-30 16:25:23,236]\u001b[0m Trial 3 finished with value: 0.8117647058823529 and parameters: {'mask_type': 'sparsemax', 'n_da': 64, 'n_steps': 1, 'gamma': 1.4, 'n_shared': 2, 'lambda_sparse': 2.313376775145369e-06, 'patienceScheduler': 5, 'patience': 25, 'epochs': 7}. Best is trial 0 with value: 0.8349186483103879.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 35 with best_epoch = 6 and best_val_1_accuracy = 0.83592\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sumin\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "\u001b[32m[I 2023-01-30 16:43:45,306]\u001b[0m Trial 4 finished with value: 0.8359198998748436 and parameters: {'mask_type': 'sparsemax', 'n_da': 56, 'n_steps': 3, 'gamma': 1.4, 'n_shared': 2, 'lambda_sparse': 1.1694994589239397e-05, 'patienceScheduler': 3, 'patience': 29, 'epochs': 80}. Best is trial 4 with value: 0.8359198998748436.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 29 with best_epoch = 13 and best_val_1_accuracy = 0.82215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sumin\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "\u001b[32m[I 2023-01-30 16:56:33,350]\u001b[0m Trial 5 finished with value: 0.8221526908635794 and parameters: {'mask_type': 'entmax', 'n_da': 56, 'n_steps': 2, 'gamma': 1.4, 'n_shared': 3, 'lambda_sparse': 4.077288952020232e-06, 'patienceScheduler': 10, 'patience': 16, 'epochs': 58}. Best is trial 4 with value: 0.8359198998748436.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 25 with best_epoch = 7 and best_val_1_accuracy = 0.82678\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sumin\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "\u001b[32m[I 2023-01-30 17:04:21,433]\u001b[0m Trial 6 finished with value: 0.8267834793491864 and parameters: {'mask_type': 'sparsemax', 'n_da': 64, 'n_steps': 1, 'gamma': 1.0, 'n_shared': 1, 'lambda_sparse': 2.909795441891099e-06, 'patienceScheduler': 9, 'patience': 18, 'epochs': 71}. Best is trial 4 with value: 0.8359198998748436.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 23 with best_epoch = 3 and best_val_1_accuracy = 0.81865\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sumin\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "\u001b[32m[I 2023-01-30 17:16:15,313]\u001b[0m Trial 7 finished with value: 0.818648310387985 and parameters: {'mask_type': 'sparsemax', 'n_da': 64, 'n_steps': 3, 'gamma': 1.2, 'n_shared': 1, 'lambda_sparse': 1.7159183839877353e-06, 'patienceScheduler': 10, 'patience': 20, 'epochs': 52}. Best is trial 4 with value: 0.8359198998748436.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop training because you reached max_epochs = 4 with best_epoch = 3 and best_val_1_accuracy = 0.81289\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sumin\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "\u001b[32m[I 2023-01-30 17:18:33,168]\u001b[0m Trial 8 finished with value: 0.8128911138923655 and parameters: {'mask_type': 'sparsemax', 'n_da': 64, 'n_steps': 3, 'gamma': 1.4, 'n_shared': 2, 'lambda_sparse': 2.4444596600446757e-05, 'patienceScheduler': 9, 'patience': 26, 'epochs': 4}. Best is trial 4 with value: 0.8359198998748436.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 26 with best_epoch = 3 and best_val_1_accuracy = 0.81527\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sumin\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "\u001b[32m[I 2023-01-30 17:31:47,669]\u001b[0m Trial 9 finished with value: 0.8152690863579475 and parameters: {'mask_type': 'sparsemax', 'n_da': 60, 'n_steps': 2, 'gamma': 1.4, 'n_shared': 3, 'lambda_sparse': 6.719314541450749e-06, 'patienceScheduler': 8, 'patience': 23, 'epochs': 70}. Best is trial 4 with value: 0.8359198998748436.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial : score 0.8359198998748436, \n",
      "params {'mask_type': 'sparsemax', 'n_da': 56, 'n_steps': 3, 'gamma': 1.4, 'n_shared': 2, 'lambda_sparse': 1.1694994589239397e-05, 'patienceScheduler': 3, 'patience': 29, 'epochs': 80}\n"
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(direction=\"maximize\", sampler = TPESampler(), study_name='TabNet optimization')\n",
    "study.optimize(lambda trial : Objective(trial, x_train, y_train, x_valid, y_valid), n_trials = 10)\n",
    "print('Best trial : score {}, \\nparams {}'.format(study.best_trial.value, study.best_trial.params))\n",
    "\n",
    "### Best trial : score 0.8359198998748436, \n",
    "### params {'mask_type': 'sparsemax', 'n_da': 56, 'n_steps': 3, 'gamma': 1.4, 'n_shared': 2, 'lambda_sparse': 1.1694994589239397e-05, 'patienceScheduler': 3, 'patience': 29, 'epochs': 80}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "10bf58d2cb1d6fe77a6d789fab1958d6f68caa67042b20c480a5cc6dc8179d5f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
